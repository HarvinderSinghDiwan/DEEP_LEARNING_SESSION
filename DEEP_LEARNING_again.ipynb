{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP_LEARNING.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPC+DrIuurdiaB0U5CFYrEU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarvinderSinghDiwan/DEEP_LEARNING_SESSION/blob/master/DEEP_LEARNING_again.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZuhdGD21p3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### NEURAL NETWORKS ###\n",
        "######ACTIVATION FUNCTION MAKES LEARN NON LINEAR RELATIONSHIP\n",
        "###### OVERFITTING\n",
        "\n",
        "######   (w.b)FORWARD PROPAGATION\n",
        "####### loss function, cost function, objective function\n",
        "##### GRADIENT DESCENT\n",
        "### USE BACKWORD PROPAGATION\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXrM91vU3BgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b9f606f-d16a-497a-9532-9ba788e61b24"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# ### Import all libraries\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "#importing all the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ### Load datasets and Normalize\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/wine.csv')\n",
        "# print(df)\n",
        "a = pd.get_dummies(df['Wine'])\n",
        "df = pd.concat([df,a],axis=1)\n",
        "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
        "y = df[[1,2,3]].values\n",
        "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Y_test,test\n",
        "\n",
        "\n",
        "# ### Explore dataset\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "print(df.tail())\n",
        "\n",
        "# ### Softmax Activation function \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ### Initialize model parameters and train model on wine dataset  \n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.07,iterations=4500,print_loss=True)\n",
        "plt.plot(losses)\n",
        "\n",
        "\n",
        "# ###  Calculate testing accuracy\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "\n",
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
            "0     1    14.23        1.71  2.43  15.6  127  ...  1.04  3.92     1065  1  0  0\n",
            "1     1    13.20        1.78  2.14  11.2  100  ...  1.05  3.40     1050  1  0  0\n",
            "2     1    13.16        2.36  2.67  18.6  101  ...  1.03  3.17     1185  1  0  0\n",
            "3     1    14.37        1.95  2.50  16.8  113  ...  0.86  3.45     1480  1  0  0\n",
            "4     1    13.24        2.59  2.87  21.0  118  ...  1.04  2.93      735  1  0  0\n",
            "\n",
            "[5 rows x 17 columns]\n",
            "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
            "173     3    13.71        5.65  2.45  20.5   95  ...  0.64  1.74      740  0  0  1\n",
            "174     3    13.40        3.91  2.48  23.0  102  ...  0.70  1.56      750  0  0  1\n",
            "175     3    13.27        4.28  2.26  20.0  120  ...  0.59  1.56      835  0  0  1\n",
            "176     3    13.17        2.59  2.37  20.0  120  ...  0.60  1.62      840  0  0  1\n",
            "177     3    14.13        4.10  2.74  24.5   96  ...  0.61  1.60      560  0  0  1\n",
            "\n",
            "[5 rows x 17 columns]\n",
            "Loss after iteration 0 : 1.3302521101015632\n",
            "Accuracy after iteration 0 : 54.929577464788736 %\n",
            "Loss after iteration 100 : 0.5609988882303469\n",
            "Accuracy after iteration 100 : 78.16901408450704 %\n",
            "Loss after iteration 200 : 0.4427870408028737\n",
            "Accuracy after iteration 200 : 85.91549295774648 %\n",
            "Loss after iteration 300 : 0.37942331236896154\n",
            "Accuracy after iteration 300 : 86.61971830985915 %\n",
            "Loss after iteration 400 : 0.3129555700952593\n",
            "Accuracy after iteration 400 : 87.32394366197182 %\n",
            "Loss after iteration 500 : 0.2859574545276481\n",
            "Accuracy after iteration 500 : 88.73239436619718 %\n",
            "Loss after iteration 600 : 0.2594958064080844\n",
            "Accuracy after iteration 600 : 90.84507042253522 %\n",
            "Loss after iteration 700 : 0.22806802491305403\n",
            "Accuracy after iteration 700 : 91.54929577464789 %\n",
            "Loss after iteration 800 : 0.19205139471957522\n",
            "Accuracy after iteration 800 : 92.25352112676056 %\n",
            "Loss after iteration 900 : 0.1738520701254879\n",
            "Accuracy after iteration 900 : 92.95774647887323 %\n",
            "Loss after iteration 1000 : 0.16119547044113872\n",
            "Accuracy after iteration 1000 : 92.95774647887323 %\n",
            "Loss after iteration 1100 : 0.15224807404243193\n",
            "Accuracy after iteration 1100 : 94.36619718309859 %\n",
            "Loss after iteration 1200 : 0.14588053029507075\n",
            "Accuracy after iteration 1200 : 94.36619718309859 %\n",
            "Loss after iteration 1300 : 0.1407836080814292\n",
            "Accuracy after iteration 1300 : 94.36619718309859 %\n",
            "Loss after iteration 1400 : 0.1359547630439743\n",
            "Accuracy after iteration 1400 : 94.36619718309859 %\n",
            "Loss after iteration 1500 : 0.13118255569674867\n",
            "Accuracy after iteration 1500 : 95.07042253521126 %\n",
            "Loss after iteration 1600 : 0.1255538423502457\n",
            "Accuracy after iteration 1600 : 95.07042253521126 %\n",
            "Loss after iteration 1700 : 0.1128418330604916\n",
            "Accuracy after iteration 1700 : 95.77464788732394 %\n",
            "Loss after iteration 1800 : 0.10527756577448089\n",
            "Accuracy after iteration 1800 : 95.77464788732394 %\n",
            "Loss after iteration 1900 : 0.10014887522875342\n",
            "Accuracy after iteration 1900 : 95.77464788732394 %\n",
            "Loss after iteration 2000 : 0.09597957913630839\n",
            "Accuracy after iteration 2000 : 96.47887323943662 %\n",
            "Loss after iteration 2100 : 0.09233840292093702\n",
            "Accuracy after iteration 2100 : 96.47887323943662 %\n",
            "Loss after iteration 2200 : 0.08899612532844468\n",
            "Accuracy after iteration 2200 : 96.47887323943662 %\n",
            "Loss after iteration 2300 : 0.08571223071254491\n",
            "Accuracy after iteration 2300 : 96.47887323943662 %\n",
            "Loss after iteration 2400 : 0.08202346539698244\n",
            "Accuracy after iteration 2400 : 96.47887323943662 %\n",
            "Loss after iteration 2500 : 0.07716176742508643\n",
            "Accuracy after iteration 2500 : 96.47887323943662 %\n",
            "Loss after iteration 2600 : 0.07119656282340739\n",
            "Accuracy after iteration 2600 : 97.1830985915493 %\n",
            "Loss after iteration 2700 : 0.06523773505918833\n",
            "Accuracy after iteration 2700 : 97.1830985915493 %\n",
            "Loss after iteration 2800 : 0.06027102676106932\n",
            "Accuracy after iteration 2800 : 97.1830985915493 %\n",
            "Loss after iteration 2900 : 0.05785855584826852\n",
            "Accuracy after iteration 2900 : 97.1830985915493 %\n",
            "Loss after iteration 3000 : 0.05622244199620002\n",
            "Accuracy after iteration 3000 : 97.88732394366197 %\n",
            "Loss after iteration 3100 : 0.054926238830759685\n",
            "Accuracy after iteration 3100 : 97.88732394366197 %\n",
            "Loss after iteration 3200 : 0.053820145770394595\n",
            "Accuracy after iteration 3200 : 97.88732394366197 %\n",
            "Loss after iteration 3300 : 0.05283387182335546\n",
            "Accuracy after iteration 3300 : 97.88732394366197 %\n",
            "Loss after iteration 3400 : 0.05192612427537473\n",
            "Accuracy after iteration 3400 : 97.88732394366197 %\n",
            "Loss after iteration 3500 : 0.051068329956812084\n",
            "Accuracy after iteration 3500 : 97.88732394366197 %\n",
            "Loss after iteration 3600 : 0.05023792519544275\n",
            "Accuracy after iteration 3600 : 97.88732394366197 %\n",
            "Loss after iteration 3700 : 0.04941834260818234\n",
            "Accuracy after iteration 3700 : 97.88732394366197 %\n",
            "Loss after iteration 3800 : 0.048616016737740374\n",
            "Accuracy after iteration 3800 : 97.88732394366197 %\n",
            "Loss after iteration 3900 : 0.04785214035859324\n",
            "Accuracy after iteration 3900 : 97.88732394366197 %\n",
            "Loss after iteration 4000 : 0.047117868020226605\n",
            "Accuracy after iteration 4000 : 97.88732394366197 %\n",
            "Loss after iteration 4100 : 0.04638461551273224\n",
            "Accuracy after iteration 4100 : 97.88732394366197 %\n",
            "Loss after iteration 4200 : 0.04562935182148354\n",
            "Accuracy after iteration 4200 : 97.88732394366197 %\n",
            "Loss after iteration 4300 : 0.04485044290732374\n",
            "Accuracy after iteration 4300 : 97.88732394366197 %\n",
            "Loss after iteration 4400 : 0.04404124463148947\n",
            "Accuracy after iteration 4400 : 97.88732394366197 %\n",
            "Testing accuracy is:  97.22222222222221%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKdxFRAKkcBcRCdD/A5IvOKSrGxymAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y_R8Ai9GZrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses=[]\n",
        "def train(model,X_,y_,learning_rate, iterations, print_loss=False):\n",
        "    # Gradient descent. Loop over epochs\n",
        "    for i in range(0, iterations):\n",
        "\n",
        "        # Forward propagation\n",
        "        cache = forward_prop(model,X_)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = backward_prop(model,cache,y_)\n",
        "        \n",
        "        # Gradient descent parameter update\n",
        "        # Assign new parameters to the model\n",
        "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
        "    \n",
        "        # Pring loss & accuracy every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            a3 = cache['a3']\n",
        "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
        "            y_hat = predict(model,X_)\n",
        "            y_true = y_.argmax(axis=1)\n",
        "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
        "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpYvHEJOIJp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ### Predict function\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXEtHaEOIOVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ### Backpropagation function\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
        "    \n",
        "    # Load forward propagation results\n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "    \n",
        "    # Get number of samples\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS1JrF0vIkoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "    # First layer weights\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "    \n",
        "    # First layer bias\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    \n",
        "    # Second layer weights\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    # Second layer bias\n",
        "    b2 = np.zeros((1, nn_hdim))\n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    b3 = np.zeros((1,nn_output_dim))\n",
        "    \n",
        "    \n",
        "    # Package and return model\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "def softmax_loss(y,y_hat):\n",
        "    # Clipping value\n",
        "    minval = 0.000000000001\n",
        "    # Number of samples\n",
        "    m = y.shape[0]\n",
        "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
        "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JIs4pe5JUtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaoPPN03Jxj9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbKNpyZLJdkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d576841f-1282-4f6f-f03a-da5676d6063b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wine</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic.acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Acl</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid.phenols</th>\n",
              "      <th>Proanth</th>\n",
              "      <th>Color.int</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD</th>\n",
              "      <th>Proline</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  ...   Hue    OD  Proline  1  2  3\n",
              "0     1    14.23        1.71  2.43  15.6  127  ...  1.04  3.92     1065  1  0  0\n",
              "1     1    13.20        1.78  2.14  11.2  100  ...  1.05  3.40     1050  1  0  0\n",
              "2     1    13.16        2.36  2.67  18.6  101  ...  1.03  3.17     1185  1  0  0\n",
              "3     1    14.37        1.95  2.50  16.8  113  ...  0.86  3.45     1480  1  0  0\n",
              "4     1    13.24        2.59  2.87  21.0  118  ...  1.04  2.93      735  1  0  0\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB4bP9DTJpg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def softmax(z):\n",
        "    #Calculate exponent term first\n",
        "    exp_scores = np.exp(z)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3TrpAxQME9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - np.power(x, 2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJExDRXMi4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POWuac54MucT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ### Update Parameters\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCr7lhPUNMlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}